#!/usr/bin/env python3
"""
Modu≈Ç do trenowania modelu NER na podstawie danych z feedback.json
"""

import json
import spacy
import random
from pathlib import Path
from collections import defaultdict
from typing import List, Dict, Tuple
from spacy.tokens import DocBin
from spacy.training import Example
import warnings
warnings.filterwarnings("ignore")


def load_feedback_data(feedback_file: str = "data/feedback.json") -> List[Dict]:
    """
    Wczytuje dane z pliku feedback.json
    
    Args:
        feedback_file: ≈öcie≈ºka do pliku z feedbackiem
        
    Returns:
        Lista feedback√≥w lub pusta lista w przypadku b≈Çƒôdu
    """
    try:
        feedback_path = Path(feedback_file)
        if not feedback_path.exists():
            print(f"‚ùå Plik {feedback_file} nie istnieje!")
            return []
        
        with open(feedback_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        if not isinstance(data, list):
            print(f"‚ùå Dane w pliku {feedback_file} nie sƒÖ listƒÖ!")
            return []
        
        print(f"‚úÖ Wczytano {len(data)} wpis√≥w z {feedback_file}")
        return data
        
    except json.JSONDecodeError as e:
        print(f"‚ùå B≈ÇƒÖd parsowania JSON w pliku {feedback_file}: {e}")
        return []
    except Exception as e:
        print(f"‚ùå Nieoczekiwany b≈ÇƒÖd podczas wczytywania {feedback_file}: {e}")
        return []


def group_by_labels(feedback_data: List[Dict]) -> Dict[str, List[str]]:
    """
    Grupuje frazy wed≈Çug etykiet
    
    Args:
        feedback_data: Lista feedback√≥w
        
    Returns:
        S≈Çownik z frazami pogrupowanymi wed≈Çug etykiet
    """
    grouped = defaultdict(list)
    
    for item in feedback_data:
        phrase = item.get('phrase', '')
        label = item.get('label', 'UNKNOWN')
        
        if phrase.strip():  # Tylko niepuste frazy
            grouped[label].append(phrase.strip())
    
    return dict(grouped)


def display_data_statistics(grouped_data: Dict[str, List[str]]) -> None:
    """
    Wy≈õwietla statystyki danych
    
    Args:
        grouped_data: Dane pogrupowane wed≈Çug etykiet
    """
    print(f"\nüìä STATYSTYKI DANYCH TRENINGOWYCH:")
    print("=" * 50)
    
    total_phrases = 0
    for label, phrases in sorted(grouped_data.items()):
        count = len(phrases)
        total_phrases += count
        
        # Ikony dla r√≥≈ºnych etykiet
        icon = {
            'GUEST': 'üë•',
            'HOST': 'üé§', 
            'OTHER': '‚ùå',
            'MAYBE': '‚ùì'
        }.get(label, 'üìù')
        
        print(f"{icon} {label:8s}: {count:4d} fraz")
        
        # Poka≈º przyk≈Çady
        if count > 0:
            examples = phrases[:3]
            examples_str = ', '.join([f'"{ex}"' for ex in examples])
            if count > 3:
                examples_str += f" ... (+{count-3} wiƒôcej)"
            print(f"   Przyk≈Çady: {examples_str}")
    
    print(f"\nüìä ≈ÅƒÑCZNIE: {total_phrases} fraz")


def prepare_training_data(grouped_data: Dict[str, List[str]]) -> List[Tuple[str, Dict]]:
    """
    Przygotowuje dane treningowe dla spaCy
    
    Args:
        grouped_data: Dane pogrupowane wed≈Çug etykiet
        
    Returns:
        Lista danych treningowych w formacie spaCy
    """
    training_data = []
    
    # U≈ºywamy tylko GUEST i HOST do treningu
    labels_to_use = ['GUEST', 'HOST']
    
    print(f"\nüîß PRZYGOTOWANIE DANYCH TRENINGOWYCH:")
    print("=" * 50)
    
    for label in labels_to_use:
        if label not in grouped_data:
            print(f"‚ö†Ô∏è  Brak danych dla etykiety {label}")
            continue
        
        phrases = grouped_data[label]
        print(f"‚úÖ Przetwarzam {len(phrases)} fraz dla etykiety {label}")
        
        for phrase in phrases:
            # Tworzymy prosty przyk≈Çad treningowy
            # Ca≈Ça fraza jest oznaczona jako dana etykieta
            text = phrase
            start = 0
            end = len(text)
            
            entities = [(start, end, label)]
            
            training_data.append((text, {"entities": entities}))
    
    print(f"üìä Przygotowano {len(training_data)} przyk≈Çad√≥w treningowych")
    return training_data


def create_spacy_examples(nlp, training_data: List[Tuple[str, Dict]]) -> List[Example]:
    """
    Tworzy przyk≈Çady spaCy z danych treningowych
    
    Args:
        nlp: Model spaCy
        training_data: Dane treningowe
        
    Returns:
        Lista przyk≈Çad√≥w spaCy
    """
    examples = []
    
    for text, annotations in training_data:
        try:
            doc = nlp.make_doc(text)
            example = Example.from_dict(doc, annotations)
            examples.append(example)
        except Exception as e:
            print(f"‚ö†Ô∏è  B≈ÇƒÖd przy tworzeniu przyk≈Çadu dla '{text}': {e}")
            continue
    
    return examples


def train_and_save_ner_model(feedback_file: str = "data/feedback.json", 
                             model_output_dir: str = "models/ner_model_2025_08_02",
                             n_iter: int = 30) -> bool:
    """
    G≈Ç√≥wna funkcja do trenowania i zapisywania modelu NER
    
    Args:
        feedback_file: ≈öcie≈ºka do pliku z feedbackiem
        model_output_dir: Folder do zapisania modelu
        n_iter: Liczba iteracji treningu
        
    Returns:
        True je≈õli trening siƒô powi√≥d≈Ç
    """
    print("üöÄ ROZPOCZYNAM TRENING MODELU NER")
    print("=" * 60)
    
    # 1. Wczytaj dane
    feedback_data = load_feedback_data(feedback_file)
    if not feedback_data:
        return False
    
    # 2. Pogrupuj wed≈Çug etykiet
    grouped_data = group_by_labels(feedback_data)
    if not grouped_data:
        print("‚ùå Brak danych do grupowania!")
        return False
    
    # 3. Wy≈õwietl statystyki
    display_data_statistics(grouped_data)
    
    # 4. Sprawd≈∫ czy mamy dane do treningu
    training_labels = ['GUEST', 'HOST']
    available_training_data = sum(len(grouped_data.get(label, [])) for label in training_labels)
    
    if available_training_data < 10:
        print(f"‚ùå Za ma≈Ço danych treningowych! Potrzeba minimum 10, mamy {available_training_data}")
        return False
    
    print(f"‚úÖ Bƒôdƒô trenowaƒá na {available_training_data} przyk≈Çadach (GUEST + HOST)")
    
    # 5. Przygotuj dane treningowe
    training_data = prepare_training_data(grouped_data)
    if not training_data:
        print("‚ùå Nie uda≈Ço siƒô przygotowaƒá danych treningowych!")
        return False
    
    # 6. Inicjalizuj model spaCy
    print(f"\nü§ñ INICJALIZACJA MODELU SPACY:")
    print("=" * 50)
    
    try:
        # Stw√≥rz pusty model polski
        nlp = spacy.blank("pl")
        
        # Dodaj komponent NER
        if "ner" not in nlp.pipe_names:
            ner = nlp.add_pipe("ner")
        else:
            ner = nlp.get_pipe("ner")
        
        # Dodaj etykiety
        for label in training_labels:
            ner.add_label(label)
        
        print(f"‚úÖ Zainicjalizowano model z etykietami: {training_labels}")
        
    except Exception as e:
        print(f"‚ùå B≈ÇƒÖd podczas inicjalizacji modelu: {e}")
        return False
    
    # 7. Przygotuj przyk≈Çady treningowe
    print(f"\nüìö PRZYGOTOWANIE PRZYK≈ÅAD√ìW:")
    print("=" * 50)
    
    try:
        examples = create_spacy_examples(nlp, training_data)
        if not examples:
            print("‚ùå Nie uda≈Ço siƒô utworzyƒá przyk≈Çad√≥w treningowych!")
            return False
        
        print(f"‚úÖ Utworzono {len(examples)} przyk≈Çad√≥w treningowych")
        
        # Pomieszaj dane
        random.shuffle(examples)
        
    except Exception as e:
        print(f"‚ùå B≈ÇƒÖd podczas przygotowania przyk≈Çad√≥w: {e}")
        return False
    
    # 8. Trenuj model
    print(f"\nüî• TRENING MODELU:")
    print("=" * 50)
    
    try:
        # Inicjalizuj wagi
        nlp.initialize()
        
        # Trening
        print(f"üöÄ Rozpoczynam trening ({n_iter} iteracji)...")
        
        losses = {}
        for iteration in range(n_iter):
            random.shuffle(examples)
            nlp.update(examples, losses=losses)
            
            if (iteration + 1) % 5 == 0:
                print(f"   Iteracja {iteration + 1:2d}/{n_iter}: loss = {losses.get('ner', 0.0):.4f}")
        
        print(f"‚úÖ Trening zako≈Ñczony! Ko≈Ñcowa strata: {losses.get('ner', 0.0):.4f}")
        
    except Exception as e:
        print(f"‚ùå B≈ÇƒÖd podczas treningu: {e}")
        return False
    
    # 9. Zapisz model
    print(f"\nüíæ ZAPISYWANIE MODELU:")
    print("=" * 50)
    
    try:
        output_path = Path(model_output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        nlp.to_disk(output_path)
        
        print(f"‚úÖ Model zapisany w: {output_path.absolute()}")
        
        # Zapisz te≈º metadane
        metadata = {
            "training_date": "2025-08-02",
            "framework": "spaCy",
            "language": "pl",
            "labels": training_labels,
            "training_examples": len(examples),
            "iterations": n_iter,
            "final_loss": losses.get('ner', 0.0),
            "data_statistics": {label: len(grouped_data.get(label, [])) for label in grouped_data.keys()}
        }
        
        metadata_file = output_path / "metadata.json"
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        
        print(f"‚úÖ Metadane zapisane w: {metadata_file}")
        
    except Exception as e:
        print(f"‚ùå B≈ÇƒÖd podczas zapisywania modelu: {e}")
        return False
    
    # 10. Test modelu
    print(f"\nüß™ TEST MODELU:")
    print("=" * 50)
    
    try:
        # Testowe frazy
        test_phrases = [
            "Rozmowa z Jakub ≈ªulczyk",
            "Go≈õƒá Piotr PajƒÖk", 
            "Host Kuba Wojew√≥dzki",
            "Wywiad z Prof. Pawe≈Ç Kaczmarczyk"
        ]
        
        for test_phrase in test_phrases:
            doc = nlp(test_phrase)
            entities = [(ent.text, ent.label_) for ent in doc.ents]
            
            if entities:
                entities_str = ", ".join([f'"{text}" ({label})' for text, label in entities])
                print(f"   '{test_phrase}' ‚Üí {entities_str}")
            else:
                print(f"   '{test_phrase}' ‚Üí (brak wykrytych nazwisk)")
        
    except Exception as e:
        print(f"‚ö†Ô∏è  B≈ÇƒÖd podczas testowania: {e}")
    
    # 11. Podsumowanie
    print(f"\nüéâ TRENING ZAKO≈ÉCZONY POMY≈öLNIE!")
    print("=" * 60)
    print(f"üìä Dane treningowe: {len(examples)} przyk≈Çad√≥w")
    print(f"üéØ Etykiety: {', '.join(training_labels)}")
    print(f"üî• Iteracje: {n_iter}")
    print(f"üìÅ Model zapisany w: {output_path.absolute()}")
    print(f"üìã U≈ºyj modelu: nlp = spacy.load('{output_path.absolute()}')")
    
    return True


def main():
    """
    G≈Ç√≥wna funkcja uruchamiajƒÖca trening
    """
    success = train_and_save_ner_model()
    
    if success:
        print(f"\n‚úÖ SUKCES! Model NER zosta≈Ç wytrenowany i zapisany.")
    else:
        print(f"\n‚ùå B≈ÅƒÑD! Nie uda≈Ço siƒô wytrenowaƒá modelu.")
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())