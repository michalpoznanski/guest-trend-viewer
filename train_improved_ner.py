#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Ulepszona wersja treningu modelu NER z lepszƒÖ precyzjƒÖ

G≈Ç√≥wne ulepszenia:
1. Inteligentne wykrywanie granic nazwisk w frazach
2. Generowanie sztucznych kontekst√≥w
3. Lepsze dane treningowe z EntityRuler
4. Pattern matching dla typowych format√≥w nazwisk

Autor: Guest Radar System  
Data: 2025-08-03
"""

import json
import spacy
import random
import re
from pathlib import Path
from spacy.training import Example
from spacy.lang.pl import Polish
from typing import List, Dict, Tuple, Set
import warnings
warnings.filterwarnings("ignore")


class ImprovedNERTrainer:
    """Ulepszona klasa do trenowania modelu NER."""
    
    def __init__(self, 
                 feedback_file: str = "data/feedback.json",
                 model_output_dir: str = "ner_model_improved"):
        """
        Inicjalizuje ulepszonego trainer'a NER.
        
        Args:
            feedback_file: Plik z danymi feedback
            model_output_dir: Katalog wyj≈õciowy dla modelu
        """
        self.feedback_file = feedback_file
        self.model_output_dir = Path(model_output_dir)
        
        # Wzorce do wykrywania nazwisk
        self.name_patterns = [
            r'\b[A-ZƒÑƒÜƒò≈Å≈É√ì≈ö≈π≈ª][a-zƒÖƒáƒô≈Ç≈Ñ√≥≈õ≈∫≈º]+\s+[A-ZƒÑƒÜƒò≈Å≈É√ì≈ö≈π≈ª][a-zƒÖƒáƒô≈Ç≈Ñ√≥≈õ≈∫≈º]+\b',  # Imiƒô Nazwisko
            r'\b[A-Z][a-z]+\s+[A-Z][a-z]+\b',  # Standardowe nazwiska
            r'\b[A-ZƒÑƒÜƒò≈Å≈É√ì≈ö≈π≈ª]{2,}\s+[A-ZƒÑƒÜƒò≈Å≈É√ì≈ö≈π≈ª]{2,}\b',  # CAPS NAZWISKA
        ]
        
        # Typowe konteksty dla nazwisk
        self.name_contexts = [
            "go≈õciem jest {name}",
            "rozmowa z {name}",
            "wywiad z {name}", 
            "prowadzi {name}",
            "autor {name}",
            "{name} opowiada",
            "{name} w studiu",
            "razem z {name}",
            "{name} i jego",
            "wed≈Çug {name}"
        ]
    
    def extract_names_from_phrase(self, phrase: str) -> List[Tuple[int, int, str]]:
        """
        Wydobywa prawdopodobne nazwiska z frazy.
        
        Args:
            phrase: Fraza do analizy
            
        Returns:
            Lista tupli (start, end, text) dla znalezionych nazwisk
        """
        names = []
        
        for pattern in self.name_patterns:
            matches = re.finditer(pattern, phrase)
            for match in matches:
                start, end = match.span()
                name_text = match.group().strip()
                
                # Filtruj oczywiste false positive
                if self._is_likely_name(name_text):
                    names.append((start, end, name_text))
        
        return names
    
    def _is_likely_name(self, text: str) -> bool:
        """
        Sprawdza czy tekst prawdopodobnie jest nazwiskiem.
        
        Args:
            text: Tekst do sprawdzenia
            
        Returns:
            True je≈õli prawdopodobnie nazwisko
        """
        # Podstawowe filtry
        if len(text) < 4 or len(text) > 50:
            return False
        
        words = text.split()
        if len(words) < 2 or len(words) > 3:
            return False
        
        # Sprawd≈∫ czy s≈Çowa wyglƒÖdajƒÖ jak nazwiska
        for word in words:
            if len(word) < 2:
                return False
            # Pierwsza litera powinna byƒá wielka
            if not word[0].isupper():
                return False
            # Pozosta≈Çe litery powinny byƒá ma≈Çe (z wyjƒÖtkiem CAPS)
            if not (word[1:].islower() or word.isupper()):
                return False
        
        # Odrzuƒá typowe false positive
        false_positives = {
            'CHCESZ NAS', 'TEN MATERIA≈Å', 'GODNY TWOJEJ', 'MO≈ªESZ POPRZEZ',
            'W PODCA≈öCIE', 'NA KANA≈Å', 'LINK W', 'DISCORD LINK'
        }
        
        if text.upper() in false_positives:
            return False
        
        return True
    
    def load_and_process_feedback(self) -> List[Tuple[str, Dict]]:
        """
        Wczytuje i przetwarza dane feedback do formatu treningowego.
        
        Returns:
            Lista danych treningowych
        """
        try:
            with open(self.feedback_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            print(f"‚úÖ Wczytano {len(data)} rekord√≥w z {self.feedback_file}")
            
            training_data = []
            processed_names = set()
            
            print(f"\nüß† PRZETWARZANIE DANYCH:")
            print("=" * 50)
            
            guest_count = 0
            maybe_count = 0
            extracted_names = 0
            
            for item in data:
                label = item.get('label', '')
                text = item.get('text') or item.get('phrase', '')
                
                if label in ['GUEST', 'MAYBE'] and text.strip():
                    text = text.strip()
                    
                    # Spr√≥buj wydobyƒá konkretne nazwiska
                    names = self.extract_names_from_phrase(text)
                    
                    if names:
                        # U≈ºyj wydobytych nazwisk
                        entities = []
                        for start, end, name_text in names:
                            entities.append((start, end, 'PERSON'))
                            processed_names.add(name_text)
                            extracted_names += 1
                        
                        training_data.append((text, {"entities": entities}))
                    else:
                        # Je≈õli brak wydobytych nazwisk, u≈ºyj ca≈Çej frazy (fallback)
                        entities = [(0, len(text), 'PERSON')]
                        training_data.append((text, {"entities": entities}))
                    
                    if label == 'GUEST':
                        guest_count += 1
                    elif label == 'MAYBE':
                        maybe_count += 1
            
            print(f"‚úÖ Przygotowano {len(training_data)} przyk≈Çad√≥w treningowych:")
            print(f"   ‚Ä¢ GUEST: {guest_count}")
            print(f"   ‚Ä¢ MAYBE: {maybe_count}")
            print(f"   ‚Ä¢ Wydobyte konkretne nazwiska: {extracted_names}")
            print(f"   ‚Ä¢ Unikalne nazwiska: {len(processed_names)}")
            
            # Generuj dodatkowe konteksty
            additional_data = self._generate_additional_contexts(processed_names)
            training_data.extend(additional_data)
            
            print(f"‚úÖ Dodano {len(additional_data)} sztucznych kontekst√≥w")
            print(f"üìä ≈ÅƒÖcznie danych treningowych: {len(training_data)}")
            
            return training_data
            
        except Exception as e:
            print(f"‚ùå B≈ÇƒÖd podczas przetwarzania danych: {e}")
            return []
    
    def _generate_additional_contexts(self, names: Set[str]) -> List[Tuple[str, Dict]]:
        """
        Generuje dodatkowe konteksty dla nazwisk.
        
        Args:
            names: Zbi√≥r nazwisk
            
        Returns:
            Lista dodatkowych przyk≈Çad√≥w treningowych
        """
        additional_data = []
        
        # Wybierz najlepsze nazwiska (kr√≥tsze, bardziej prawdopodobne)
        good_names = []
        for name in names:
            if len(name.split()) == 2 and len(name) < 25 and self._is_likely_name(name):
                good_names.append(name)
        
        # Ogranicz do 20 najlepszych nazwisk
        selected_names = random.sample(good_names, min(20, len(good_names)))
        
        for name in selected_names:
            for context_template in self.name_contexts[:5]:  # U≈ºyj 5 kontekst√≥w
                # Stw√≥rz kontekst
                full_text = context_template.format(name=name)
                
                # Znajd≈∫ pozycjƒô nazwiska w kontek≈õcie
                name_start = full_text.find(name)
                name_end = name_start + len(name)
                
                entities = [(name_start, name_end, 'PERSON')]
                additional_data.append((full_text, {"entities": entities}))
        
        return additional_data
    
    def create_spacy_model(self) -> spacy.Language:
        """
        Tworzy model spaCy z EntityRuler i NER.
        
        Returns:
            Model spaCy
        """
        print(f"\nü§ñ TWORZENIE MODELU SPACY:")
        print("=" * 50)
        
        # Utw√≥rz pusty model polski
        nlp = spacy.blank("pl")
        
        # Dodaj komponent NER najpierw
        if "ner" not in nlp.pipe_names:
            ner = nlp.add_pipe("ner")
        else:
            ner = nlp.get_pipe("ner")
        
        # Dodaj EntityRuler przed NER
        if "entity_ruler" not in nlp.pipe_names:
            ruler = nlp.add_pipe("entity_ruler", before="ner")
        
        # Dodaj etykietƒô PERSON
        ner.add_label("PERSON")
        
        # Dodaj wzorce do EntityRuler
        patterns = [
            {"label": "PERSON", "pattern": [
                {"SHAPE": "Xxxxx"}, {"SHAPE": "Xxxxx"}
            ]},
            {"label": "PERSON", "pattern": [
                {"IS_TITLE": True}, {"IS_TITLE": True}
            ]}
        ]
        
        ruler.add_patterns(patterns)
        
        print(f"‚úÖ Utworzono model z komponentami: {nlp.pipe_names}")
        print(f"‚úÖ Etykiety NER: ['PERSON']")
        print(f"‚úÖ Wzorce EntityRuler: {len(patterns)}")
        
        return nlp
    
    def train_model(self, nlp: spacy.Language, training_data: List[Tuple[str, Dict]], 
                   n_iter: int = 20) -> spacy.Language:
        """
        Trenuje model NER.
        
        Args:
            nlp: Model spaCy
            training_data: Dane treningowe
            n_iter: Liczba iteracji treningu
            
        Returns:
            Wytrenowany model
        """
        print(f"\nüî• TRENING MODELU:")
        print("=" * 50)
        
        # Przygotuj przyk≈Çady treningowe
        examples = []
        for text, annotations in training_data:
            try:
                doc = nlp.make_doc(text)
                example = Example.from_dict(doc, annotations)
                examples.append(example)
            except Exception as e:
                print(f"‚ö†Ô∏è B≈ÇƒÖd dla przyk≈Çadu '{text[:30]}...': {e}")
                continue
        
        print(f"‚úÖ Przygotowano {len(examples)} przyk≈Çad√≥w do treningu")
        
        # Inicjalizuj tylko NER (EntityRuler jest ju≈º gotowy)
        nlp.initialize(lambda: examples)
        
        # Wy≈ÇƒÖcz EntityRuler podczas treningu NER
        with nlp.disable_pipes("entity_ruler"):
            print(f"üöÄ Rozpoczynam trening NER ({n_iter} iteracji)...")
            
            losses = {}
            for iteration in range(n_iter):
                random.shuffle(examples)
                nlp.update(examples, losses=losses)
                
                # Wy≈õwietl postƒôp co 4 iteracje
                if (iteration + 1) % 4 == 0:
                    loss = losses.get('ner', 0)
                    print(f"   Iteracja {iteration + 1:2d}/{n_iter}: loss = {loss:.4f}")
        
        final_loss = losses.get('ner', 0)
        print(f"‚úÖ Trening zako≈Ñczony! Ko≈Ñcowa strata: {final_loss:.4f}")
        
        return nlp
    
    def test_model(self, nlp: spacy.Language) -> None:
        """
        Testuje wytrenowany model.
        
        Args:
            nlp: Wytrenowany model
        """
        print(f"\nüß™ TEST MODELU:")
        print("=" * 50)
        
        test_texts = [
            "W podca≈õcie go≈õciem jest Jakub ≈ªulczyk",
            "Rozmowa z Krzysztofem Stanowskim i AnnƒÖ KowalskƒÖ",
            "Adam Ma≈Çysz opowiada o skokach narciarskich",
            "Program prowadzi Kuba Wojew√≥dzki wraz z Marcinem Prokopem",
            "Wywiad z prezesem, Janem Nowakiem",
            "Piotr PajƒÖk premiera nowej ksiƒÖ≈ºki"
        ]
        
        for i, text in enumerate(test_texts, 1):
            print(f"\n{i}. Tekst: \"{text}\"")
            doc = nlp(text)
            
            if doc.ents:
                print("   Wykryte encje:")
                for ent in doc.ents:
                    print(f"     ‚Ä¢ \"{ent.text}\" ‚Üí {ent.label_}")
            else:
                print("   ‚ùå Brak wykrytych encji")
    
    def save_model(self, nlp: spacy.Language) -> bool:
        """
        Zapisuje wytrenowany model.
        
        Args:
            nlp: Wytrenowany model
            
        Returns:
            True je≈õli uda≈Ço siƒô zapisaƒá
        """
        print(f"\nüíæ ZAPISYWANIE MODELU:")
        print("=" * 50)
        
        try:
            # Utw√≥rz katalog je≈õli nie istnieje
            self.model_output_dir.mkdir(parents=True, exist_ok=True)
            
            # Zapisz model
            nlp.to_disk(self.model_output_dir)
            
            print(f"‚úÖ Model zapisany w: {self.model_output_dir.absolute()}")
            
            # Wy≈õwietl zawarto≈õƒá katalogu
            files = list(self.model_output_dir.iterdir())
            print(f"üìÅ Pliki modelu: {[f.name for f in files]}")
            
            return True
            
        except Exception as e:
            print(f"‚ùå B≈ÇƒÖd podczas zapisywania modelu: {e}")
            return False
    
    def run_training(self) -> bool:
        """
        Uruchamia pe≈Çny proces treningu.
        
        Returns:
            True je≈õli trening siƒô powi√≥d≈Ç
        """
        print("üöÄ ROZPOCZYNAM TRENING ULEPSZONEGO MODELU NER")
        print("=" * 60)
        
        # 1. Wczytaj i przetw√≥rz dane
        training_data = self.load_and_process_feedback()
        if not training_data:
            print("‚ùå Brak danych do treningu")
            return False
        
        # 2. Utw√≥rz model
        nlp = self.create_spacy_model()
        
        # 3. Trenuj model
        nlp = self.train_model(nlp, training_data)
        
        # 4. Zapisz model
        if not self.save_model(nlp):
            print("‚ùå Nie uda≈Ço siƒô zapisaƒá modelu")
            return False
        
        # 5. Testuj model
        self.test_model(nlp)
        
        print(f"\nüéâ TRENING ZAKO≈ÉCZONY POMY≈öLNIE!")
        print(f"üìÅ Model zapisany w: {self.model_output_dir}")
        
        return True


def main():
    """G≈Ç√≥wna funkcja."""
    trainer = ImprovedNERTrainer()
    success = trainer.run_training()
    
    if success:
        print("\n‚úÖ Ulepszon√Ω model NER zosta≈Ç pomy≈õlnie wytrenowany!")
    else:
        print("\n‚ùå Trening nieudany!")


if __name__ == "__main__":
    main()